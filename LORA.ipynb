{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a38803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from previous_chapters import (\n",
    "    download_and_unzip_spam_data,\n",
    "    create_balanced_dataset,\n",
    "    random_split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff2d7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "url = \\\n",
    "\"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "\n",
    "df = pd.read_csv(\n",
    "    data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"]\n",
    ")\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "\n",
    "train_df.to_csv(\"train.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5af0fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54ff3860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import tiktoken\n",
    "from previous_chapters import SpamDataset\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(\"train.csv\", max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = SpamDataset(\"validation.csv\",\n",
    "    max_length=train_dataset.max_length, tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    \"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fdb1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d10e4cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f2f48f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95ce2088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel\n",
    "from previous_chapters import load_weights_into_gpt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "973582f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size, models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f3f2b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import generate_text_simple\n",
    "from previous_chapters import text_to_token_ids, token_ids_to_text\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c90bd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the model for classification fine-tuning by replacing output layer\n",
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=768, out_features=num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "869616d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import calc_accuracy_loader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "    train_loader, model, device, num_batches=10\n",
    ")\n",
    "val_accuracy = calc_accuracy_loader(\n",
    "    val_loader, model, device, num_batches=10\n",
    ")\n",
    "test_accuracy = calc_accuracy_loader(\n",
    "    test_loader, model, device, num_batches=10\n",
    ")\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee241d",
   "metadata": {},
   "source": [
    "## Parameter-efficient fine-tuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a400fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# The rank governs the inner dimension of matrices A and B\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6939978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e27ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:\n",
    "            replace_linear_with_lora(module, rank, alpha)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e653456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters before: 124,441,346\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters before: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6396cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters after: 0\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters after: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b90e1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable LoRA parameters: 2,666,528\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fdbef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): LinearWithLoRA(\n",
      "    (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35e2a53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "    train_loader, model, device, num_batches=10\n",
    ")\n",
    "val_accuracy = calc_accuracy_loader(\n",
    "    val_loader, model, device, num_batches=10\n",
    ")\n",
    "test_accuracy = calc_accuracy_loader(\n",
    "    test_loader, model, device, num_batches=10\n",
    ")\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95890112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 3.820, Val loss 3.462\n",
      "Ep 1 (Step 000050): Train loss 0.396, Val loss 0.364\n",
      "Ep 1 (Step 000100): Train loss 0.111, Val loss 0.229\n",
      "Training accuracy: 97.50% | Validation accuracy: 95.00%\n",
      "Ep 2 (Step 000150): Train loss 0.135, Val loss 0.073\n",
      "Ep 2 (Step 000200): Train loss 0.008, Val loss 0.051\n",
      "Ep 2 (Step 000250): Train loss 0.022, Val loss 0.178\n",
      "Training accuracy: 97.50% | Validation accuracy: 97.50%\n",
      "Ep 3 (Step 000300): Train loss 0.082, Val loss 0.053\n",
      "Ep 3 (Step 000350): Train loss 0.019, Val loss 0.130\n",
      "Training accuracy: 100.00% | Validation accuracy: 90.00%\n",
      "Ep 4 (Step 000400): Train loss 0.004, Val loss 0.142\n",
      "Ep 4 (Step 000450): Train loss 0.005, Val loss 0.023\n",
      "Ep 4 (Step 000500): Train loss 0.000, Val loss 0.119\n",
      "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
      "Ep 5 (Step 000550): Train loss 0.009, Val loss 0.183\n",
      "Ep 5 (Step 000600): Train loss 0.017, Val loss 0.015\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed in 8.80 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning a model with LoRA layers\n",
    "\n",
    "import time\n",
    "from previous_chapters import train_classifier_simple\n",
    "\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = \\\n",
    "    train_classifier_simple(\n",
    "        model, train_loader, val_loader, optimizer, device,\n",
    "        num_epochs=num_epochs, eval_freq=50, eval_iter=5)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91e667c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_chapters import plot_values\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "plot_values(\n",
    "epochs_tensor, examples_seen_tensor,\n",
    "train_losses, val_losses, label=\"loss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1204cd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 99.90%\n",
      "Validation accuracy: 96.64%\n",
      "Test accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e01fcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(\n",
    "    text, model, tokenizer, device, max_length=None,\n",
    "    pad_token_id=50256):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.pos_emb.weight.shape[1]\n",
    "    \n",
    "    input_ids = input_ids[:min(\n",
    "        max_length, supported_context_length\n",
    "    )]\n",
    "    \n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    \n",
    "    input_tensor = torch.tensor(\n",
    "        input_ids, device=device\n",
    "    ).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88248395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5902adf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not spam\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "\"Hey, just wanted to check if we're still on\"\n",
    "\" for dinner tonight? Let me know!\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df6464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
